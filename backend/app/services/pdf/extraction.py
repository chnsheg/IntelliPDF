"""
PDF å†…å®¹æå–æœåŠ¡
æä¾›ç»“æ„åŒ–çš„æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾ç‰‡æå–åŠŸèƒ½
"""
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import re

from loguru import logger

from .parser import PDFParser
from .cache import get_pdf_cache
from ...core.exceptions import PDFProcessingError


class PDFExtractor:
    """PDF å†…å®¹æå–å™¨"""

    def __init__(self, pdf_path: str | Path, use_cache: bool = True):
        """
        åˆå§‹åŒ–æå–å™¨

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.parser = PDFParser(pdf_path, use_cache=use_cache)
        self.pdf_path = Path(pdf_path)
        self.use_cache = use_cache
        self.cache = get_pdf_cache() if use_cache else None

    def extract_structured_text(
        self,
        preserve_formatting: bool = True,
        clean_text: bool = True
    ) -> List[Dict[str, Any]]:
        """
        æå–ç»“æ„åŒ–æ–‡æœ¬ï¼Œæ¯é¡µä½œä¸ºä¸€ä¸ªç»“æ„åŒ–å•å…ƒï¼ˆæ”¯æŒç¼“å­˜ï¼‰

        Args:
            preserve_formatting: æ˜¯å¦ä¿ç•™æ ¼å¼ï¼ˆæ¢è¡Œã€ç¼©è¿›ç­‰ï¼‰
            clean_text: æ˜¯å¦æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰

        Returns:
            ç»“æ„åŒ–æ–‡æœ¬åˆ—è¡¨
        """
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if self.use_cache and self.cache:
            cached_text = self.cache.load_structured_text(self.pdf_path)
            if cached_text:
                logger.info(
                    f"âœ… Loaded structured text from cache ({len(cached_text)} pages)")
                return cached_text

        try:
            text_by_page = self.parser.extract_text(engine="pymupdf")
            dimensions = self.parser.get_page_dimensions()

            structured_content = []

            for page_num, raw_text in text_by_page.items():
                # æ¸…ç†æ–‡æœ¬
                text = self._clean_text(raw_text) if clean_text else raw_text

                if not text.strip():
                    continue

                # æ„å»ºç»“æ„åŒ–æ•°æ®
                page_data = {
                    'page_number': page_num + 1,  # ä» 1 å¼€å§‹è®¡æ•°
                    'page_index': page_num,
                    'text': text,
                    'char_count': len(text),
                    'word_count': len(text.split()),
                    'line_count': len(text.split('\n')),
                    'dimensions': dimensions.get(page_num, {})
                }

                # æå–é¡µé¢æ ‡é¢˜ï¼ˆå¯å‘å¼ï¼šç¬¬ä¸€è¡Œéç©ºæ–‡æœ¬ï¼‰
                lines = [line.strip()
                         for line in text.split('\n') if line.strip()]
                if lines:
                    page_data['first_line'] = lines[0]
                    page_data['has_content'] = True
                else:
                    page_data['has_content'] = False

                structured_content.append(page_data)

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.use_cache and self.cache:
                self.cache.save_structured_text(
                    self.pdf_path, structured_content)
                logger.info(f"ğŸ’¾ Saved structured text to cache")

            logger.info(
                f"Extracted structured text from {len(structured_content)} pages")
            return structured_content

        except Exception as e:
            logger.error(f"Error extracting structured text: {e}")
            raise PDFProcessingError(
                f"Failed to extract structured text: {str(e)}")

    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œï¼ˆè¿ç»­3ä¸ªä»¥ä¸Šæ¢è¡Œç¬¦å‹ç¼©ä¸º2ä¸ªï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)

        # ç§»é™¤è¡Œå°¾ç©ºç™½
        text = '\n'.join(line.rstrip() for line in text.split('\n'))

        # ç§»é™¤ PDF ç‰¹æ®Šå­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]', '', text)

        return text

    def extract_sections(
        self,
        title_patterns: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        æå–æ–‡æ¡£ç« èŠ‚ï¼ˆåŸºäºæ ‡é¢˜æ¨¡å¼ï¼‰

        Args:
            title_patterns: æ ‡é¢˜åŒ¹é…æ¨¡å¼åˆ—è¡¨ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        try:
            # é»˜è®¤æ ‡é¢˜æ¨¡å¼
            if title_patterns is None:
                title_patterns = [
                    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s+.+$',  # ç¬¬Xç« 
                    r'^\d+\.\s+.+$',  # 1. æ ‡é¢˜
                    r'^\d+\.\d+\s+.+$',  # 1.1 æ ‡é¢˜
                    r'^[A-Z\u4e00-\u9fa5]{2,20}$',  # å…¨å¤§å†™æˆ–ä¸­æ–‡æ ‡é¢˜
                ]

            compiled_patterns = [re.compile(
                pattern, re.MULTILINE) for pattern in title_patterns]

            text_by_page = self.parser.extract_text(engine="pymupdf")
            sections = []
            current_section = None

            for page_num in sorted(text_by_page.keys()):
                text = text_by_page[page_num]
                lines = text.split('\n')

                for line in lines:
                    line = line.strip()
                    if not line:
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ ‡é¢˜æ¨¡å¼
                    is_title = False
                    for pattern in compiled_patterns:
                        if pattern.match(line):
                            is_title = True
                            break

                    if is_title:
                        # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                        if current_section and current_section.get('content'):
                            sections.append(current_section)

                        # å¼€å§‹æ–°ç« èŠ‚
                        current_section = {
                            'title': line,
                            'start_page': page_num + 1,
                            'content': []
                        }
                    elif current_section is not None:
                        # æ·»åŠ å†…å®¹åˆ°å½“å‰ç« èŠ‚
                        current_section['content'].append(line)

            # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
            if current_section and current_section.get('content'):
                sections.append(current_section)

            # è®¡ç®—ç« èŠ‚ç»Ÿè®¡
            for section in sections:
                section['content_text'] = '\n'.join(section['content'])
                section['char_count'] = len(section['content_text'])
                section['word_count'] = len(section['content_text'].split())
                del section['content']  # ç§»é™¤ä¸´æ—¶æ•°ç»„

            logger.info(f"Extracted {len(sections)} sections")
            return sections

        except Exception as e:
            logger.error(f"Error extracting sections: {e}")
            raise PDFProcessingError(f"Failed to extract sections: {str(e)}")

    def extract_tables_formatted(self) -> List[Dict[str, Any]]:
        """
        æå–æ ¼å¼åŒ–çš„è¡¨æ ¼æ•°æ®

        Returns:
            è¡¨æ ¼åˆ—è¡¨ï¼Œæ¯ä¸ªè¡¨æ ¼åŒ…å«å…ƒæ•°æ®å’Œæ•°æ®
        """
        try:
            tables_by_page = self.parser.extract_tables()
            formatted_tables = []

            for page_num, page_tables in tables_by_page.items():
                for table_index, table_data in enumerate(page_tables):
                    if not table_data:
                        continue

                    # åˆ†æè¡¨æ ¼ç»“æ„
                    row_count = len(table_data)
                    col_count = len(table_data[0]) if table_data else 0

                    # æå–è¡¨å¤´ï¼ˆå‡è®¾ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´ï¼‰
                    headers = table_data[0] if table_data else []
                    data_rows = table_data[1:] if len(table_data) > 1 else []

                    formatted_table = {
                        'page_number': page_num + 1,
                        'page_index': page_num,
                        'table_index': table_index,
                        'row_count': row_count,
                        'column_count': col_count,
                        'headers': headers,
                        'data': data_rows,
                        'has_headers': bool(headers and any(h for h in headers if h)),
                    }

                    formatted_tables.append(formatted_table)

            logger.info(f"Extracted {len(formatted_tables)} formatted tables")
            return formatted_tables

        except Exception as e:
            logger.error(f"Error extracting formatted tables: {e}")
            raise PDFProcessingError(
                f"Failed to extract formatted tables: {str(e)}")

    def extract_metadata_enhanced(self) -> Dict[str, Any]:
        """
        æå–å¢å¼ºçš„å…ƒæ•°æ®ï¼ˆåŒ…æ‹¬å†…å®¹åˆ†æï¼‰

        Returns:
            å¢å¼ºå…ƒæ•°æ®å­—å…¸
        """
        try:
            # åŸºç¡€å…ƒæ•°æ®
            metadata = self.parser.get_metadata()

            # å†…å®¹ç»Ÿè®¡
            text_by_page = self.parser.extract_text(engine="pymupdf")
            all_text = ' '.join(text_by_page.values())

            # è¯­è¨€æ£€æµ‹ï¼ˆç®€å•çš„å¯å‘å¼ï¼‰
            chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', all_text))
            english_chars = len(re.findall(r'[a-zA-Z]', all_text))
            total_chars = len(all_text.strip())

            if chinese_chars > english_chars:
                language = 'zh'
                language_confidence = chinese_chars / total_chars if total_chars > 0 else 0
            else:
                language = 'en'
                language_confidence = english_chars / total_chars if total_chars > 0 else 0

            # å†…å®¹ç±»å‹æ£€æµ‹
            has_tables = bool(self.parser.extract_tables())
            has_images = bool(self.parser.extract_images())

            # å¢å¼ºå…ƒæ•°æ®
            metadata['content_analysis'] = {
                'total_characters': total_chars,
                'chinese_characters': chinese_chars,
                'english_characters': english_chars,
                'detected_language': language,
                'language_confidence': round(language_confidence, 3),
                'has_tables': has_tables,
                'has_images': has_images,
                'avg_chars_per_page': total_chars // metadata['page_count'] if metadata['page_count'] > 0 else 0
            }

            logger.info(f"Enhanced metadata: {metadata['content_analysis']}")
            return metadata

        except Exception as e:
            logger.error(f"Error extracting enhanced metadata: {e}")
            raise PDFProcessingError(
                f"Failed to extract enhanced metadata: {str(e)}")

    def extract_all(self) -> Dict[str, Any]:
        """
        æå–æ‰€æœ‰å†…å®¹ï¼ˆä¸€ç«™å¼æå–ï¼‰

        Returns:
            åŒ…å«æ‰€æœ‰æå–å†…å®¹çš„å­—å…¸
        """
        try:
            logger.info(
                f"Starting comprehensive extraction for {self.pdf_path.name}")

            result = {
                'metadata': self.extract_metadata_enhanced(),
                'structured_text': self.extract_structured_text(),
                'sections': self.extract_sections(),
                'tables': self.extract_tables_formatted(),
            }

            # ç»Ÿè®¡æ‘˜è¦
            result['summary'] = {
                'total_pages': result['metadata']['page_count'],
                'pages_with_text': len(result['structured_text']),
                'total_sections': len(result['sections']),
                'total_tables': len(result['tables']),
                'total_characters': result['metadata']['content_analysis']['total_characters'],
                'detected_language': result['metadata']['content_analysis']['detected_language']
            }

            logger.info(
                f"Comprehensive extraction complete: {result['summary']}")
            return result

        except Exception as e:
            logger.error(f"Error in comprehensive extraction: {e}")
            raise PDFProcessingError(
                f"Failed to extract all content: {str(e)}")
