"""
ç« èŠ‚çº§åˆ«çš„æ™ºèƒ½åˆ†å—å™¨
é’ˆå¯¹æŠ€æœ¯æ–‡æ¡£ï¼ˆå¦‚ Linux æ•™ç¨‹ï¼‰ä¼˜åŒ–ï¼ŒæŒ‰ç« èŠ‚å’Œå°èŠ‚è¿›è¡Œåˆ†å—
"""
from typing import List, Dict, Any, Optional
from pathlib import Path
import re

from loguru import logger

from .cache import get_pdf_cache


class SectionChunker:
    """ç« èŠ‚çº§åˆ«çš„æ™ºèƒ½åˆ†å—å™¨"""

    def __init__(self, use_cache: bool = True):
        """
        åˆå§‹åŒ–ç« èŠ‚åˆ†å—å™¨

        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.use_cache = use_cache
        self.cache = get_pdf_cache() if use_cache else None

        # ç« èŠ‚æ ‡é¢˜æ¨¡å¼ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        self.chapter_patterns = [
            r'^ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*ç« ',  # ç¬¬ä¸€ç« ã€ç¬¬1ç« 
            r'^Chapter\s+\d+',  # Chapter 1
            r'^\d+\.\s+[A-Z]',  # 1. Introduction
            r'^[A-Z][A-Z\s]+$',  # å…¨å¤§å†™æ ‡é¢˜
        ]

        # å°èŠ‚æ ‡é¢˜æ¨¡å¼
        self.section_patterns = [
            r'^\d+\.\d+\s+',  # 1.1
            r'^\d+\.\d+\.\d+\s+',  # 1.1.1
            r'^[A-Z]\.\s+',  # A.
            r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)',  # (ä¸€) (1)
        ]

        logger.info("Initialized SectionChunker")

    def chunk_by_sections(
        self,
        structured_text: List[Dict[str, Any]],
        pdf_path: Optional[Path] = None,
        min_section_length: int = 100,
        max_section_length: int = 5000
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰ç« èŠ‚å’Œå°èŠ‚è¿›è¡Œåˆ†å—ï¼ˆæ”¯æŒç¼“å­˜ï¼‰

        Args:
            structured_text: ç»“æ„åŒ–æ–‡æœ¬åˆ—è¡¨ï¼ˆæ¯é¡µä¸€ä¸ªå…ƒç´ ï¼‰
            pdf_path: PDF æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºç¼“å­˜ï¼‰
            min_section_length: æœ€å°å°èŠ‚é•¿åº¦
            max_section_length: æœ€å¤§å°èŠ‚é•¿åº¦

        Returns:
            ç« èŠ‚åˆ†å—åˆ—è¡¨
        """
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if self.use_cache and self.cache and pdf_path:
            cached_chunks = self.cache.load_chunks(
                pdf_path, chunk_strategy="section")
            if cached_chunks:
                logger.info(
                    f"âœ… Loaded section chunks from cache ({len(cached_chunks)} chunks)")
                return cached_chunks

        try:
            chunks = []
            current_chapter = None
            current_section = None
            current_content = []
            current_pages = []
            chunk_index = 0

            # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬
            all_text = ""
            page_map = {}  # å­—ç¬¦ä½ç½® -> é¡µç æ˜ å°„
            char_pos = 0

            for page_data in structured_text:
                page_num = page_data['page_index']
                page_text = page_data['text']

                # è®°å½•æ¯ä¸ªå­—ç¬¦çš„é¡µç 
                for char in page_text:
                    page_map[char_pos] = page_num
                    char_pos += 1

                all_text += page_text + "\n\n"
                char_pos += 2  # æ·»åŠ çš„ \n\n

            # æŒ‰è¡Œåˆ†å‰²
            lines = all_text.split('\n')
            line_start_pos = 0

            for line in lines:
                line = line.strip()

                if not line:
                    line_start_pos += 1
                    continue

                # æ£€æµ‹æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜
                is_chapter = self._is_chapter_title(line)
                is_section = self._is_section_title(line)

                if is_chapter:
                    # ä¿å­˜ä¸Šä¸€ä¸ªåˆ†å—
                    if current_content and len(''.join(current_content)) >= min_section_length:
                        chunk = self._create_chunk(
                            content=''.join(current_content),
                            chapter=current_chapter,
                            section=current_section,
                            pages=current_pages,
                            index=chunk_index
                        )
                        chunks.append(chunk)
                        chunk_index += 1

                    # å¼€å§‹æ–°ç« èŠ‚
                    current_chapter = line
                    current_section = None
                    current_content = [f"# {line}\n\n"]
                    current_pages = [page_map.get(line_start_pos, 0)]

                elif is_section:
                    # ä¿å­˜ä¸Šä¸€ä¸ªå°èŠ‚
                    if current_content and len(''.join(current_content)) >= min_section_length:
                        chunk = self._create_chunk(
                            content=''.join(current_content),
                            chapter=current_chapter,
                            section=current_section,
                            pages=current_pages,
                            index=chunk_index
                        )
                        chunks.append(chunk)
                        chunk_index += 1

                        # å¦‚æœå½“å‰å—å¤ªé•¿ï¼Œåˆ†å‰²
                        if chunk['char_count'] > max_section_length:
                            chunks.extend(self._split_large_chunk(
                                chunk, max_section_length))

                    # å¼€å§‹æ–°å°èŠ‚
                    current_section = line
                    current_content = [f"## {line}\n\n"]
                    current_pages = [page_map.get(line_start_pos, 0)]

                else:
                    # æ™®é€šå†…å®¹
                    current_content.append(line + '\n')
                    page_num = page_map.get(line_start_pos, 0)
                    if not current_pages or current_pages[-1] != page_num:
                        current_pages.append(page_num)

                line_start_pos += len(line) + 1

            # ä¿å­˜æœ€åä¸€ä¸ªåˆ†å—
            if current_content and len(''.join(current_content)) >= min_section_length:
                chunk = self._create_chunk(
                    content=''.join(current_content),
                    chapter=current_chapter,
                    section=current_section,
                    pages=current_pages,
                    index=chunk_index
                )
                chunks.append(chunk)

            logger.info(f"Created {len(chunks)} section-based chunks")

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            for chunk in chunks:
                chunk['total_chunks'] = len(chunks)

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.use_cache and self.cache and pdf_path:
                self.cache.save_chunks(
                    pdf_path, chunks, chunk_strategy="section")
                logger.info(f"ğŸ’¾ Saved section chunks to cache")

            return chunks

        except Exception as e:
            logger.error(f"Error in section chunking: {e}")
            return []

    def _is_chapter_title(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜"""
        for pattern in self.chapter_patterns:
            if re.match(pattern, line):
                return True
        return False

    def _is_section_title(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå°èŠ‚æ ‡é¢˜"""
        for pattern in self.section_patterns:
            if re.match(pattern, line):
                return True
        return False

    def _create_chunk(
        self,
        content: str,
        chapter: Optional[str],
        section: Optional[str],
        pages: List[int],
        index: int
    ) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†å—"""
        return {
            'chunk_index': index,
            'text': content.strip(),
            'char_count': len(content),
            'word_count': len(content.split()),
            'chapter': chapter or "Unknown Chapter",
            'section': section or "Introduction",
            'page_numbers': list(set(pages)),
            'page_range': f"{min(pages)}-{max(pages)}" if pages else "0",
            'metadata': {
                'chapter': chapter,
                'section': section,
                'pages': sorted(set(pages)),
                'type': 'section_chunk'
            }
        }

    def _split_large_chunk(
        self,
        chunk: Dict[str, Any],
        max_length: int
    ) -> List[Dict[str, Any]]:
        """åˆ†å‰²è¿‡å¤§çš„å—"""
        content = chunk['text']
        if len(content) <= max_length:
            return [chunk]

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        sub_chunks = []
        current_chunk = []
        current_length = 0

        for para in paragraphs:
            para_length = len(para)

            if current_length + para_length > max_length and current_chunk:
                # åˆ›å»ºå­å—
                sub_chunk = chunk.copy()
                sub_chunk['text'] = '\n\n'.join(current_chunk)
                sub_chunk['char_count'] = current_length
                sub_chunk['is_split'] = True
                sub_chunks.append(sub_chunk)

                current_chunk = [para]
                current_length = para_length
            else:
                current_chunk.append(para)
                current_length += para_length + 2  # +2 for \n\n

        # æ·»åŠ æœ€åä¸€ä¸ªå­å—
        if current_chunk:
            sub_chunk = chunk.copy()
            sub_chunk['text'] = '\n\n'.join(current_chunk)
            sub_chunk['char_count'] = current_length
            sub_chunk['is_split'] = True
            sub_chunks.append(sub_chunk)

        logger.info(f"Split large chunk into {len(sub_chunks)} sub-chunks")
        return sub_chunks

    def get_chunk_summary(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è·å–åˆ†å—æ‘˜è¦ç»Ÿè®¡

        Args:
            chunks: åˆ†å—åˆ—è¡¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        if not chunks:
            return {}

        chapters = set()
        sections = set()
        total_chars = 0
        total_pages = set()

        for chunk in chunks:
            if chunk.get('chapter'):
                chapters.add(chunk['chapter'])
            if chunk.get('section'):
                sections.add(chunk['section'])
            total_chars += chunk.get('char_count', 0)
            total_pages.update(chunk.get('page_numbers', []))

        return {
            'total_chunks': len(chunks),
            'total_chapters': len(chapters),
            'total_sections': len(sections),
            'total_characters': total_chars,
            'total_pages': len(total_pages),
            'avg_chunk_size': total_chars // len(chunks) if chunks else 0,
            'chapters': sorted(list(chapters)),
            'pages_covered': sorted(list(total_pages))
        }
