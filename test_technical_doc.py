"""
æµ‹è¯•æŠ€æœ¯æ–‡æ¡£å¤„ç†æµç¨‹
ä½¿ç”¨ Linuxæ•™ç¨‹.pdf è¿›è¡Œå®Œæ•´çš„ç« èŠ‚çº§åˆ†å—å’ŒRAGæµ‹è¯•
"""
from loguru import logger
from app.services.ai import EmbeddingsService, RetrievalService, TechnicalDocRAG
from app.services.pdf import PDFExtractor, TechnicalDocChunker
import sys
import os
import asyncio
from pathlib import Path

# è®¾ç½®å·¥ä½œç›®å½•
original_dir = os.getcwd()
backend_dir = Path(__file__).parent / "backend"
os.chdir(backend_dir)
sys.path.insert(0, str(backend_dir))


async def test_technical_doc_pipeline():
    """æµ‹è¯•æŠ€æœ¯æ–‡æ¡£å¤„ç†çš„å®Œæ•´æµç¨‹"""
    print("\n" + "="*70)
    print("ğŸš€ æŠ€æœ¯æ–‡æ¡£å¤„ç†æµ‹è¯• - Linuxæ•™ç¨‹.pdf")
    print("="*70)

    pdf_path = Path(original_dir) / "Linuxæ•™ç¨‹.pdf"

    if not pdf_path.exists():
        print(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    document_id = "linux_tutorial"

    try:
        # ========== é˜¶æ®µ 1: PDF è§£æ ==========
        print("\n" + "="*70)
        print("ğŸ“„ é˜¶æ®µ 1: PDF è§£æ")
        print("="*70)

        print("\n1ï¸âƒ£  æå–æ–‡æ¡£ç»“æ„...")
        extractor = PDFExtractor(pdf_path)
        metadata = extractor.extract_metadata_enhanced()
        print(f"   âœ“ æ€»é¡µæ•°: {metadata['page_count']}")
        print(
            f"   âœ“ æ€»å­—ç¬¦æ•°: {metadata['content_analysis']['total_characters']:,}")
        print(
            f"   âœ“ æ£€æµ‹è¯­è¨€: {metadata['content_analysis']['detected_language']}")

        # ========== é˜¶æ®µ 2: ç« èŠ‚çº§åˆ†å— ==========
        print("\n" + "="*70)
        print("âœ‚ï¸  é˜¶æ®µ 2: ç« èŠ‚çº§æ™ºèƒ½åˆ†å—")
        print("="*70)

        print("\n1ï¸âƒ£  åˆ›å»ºæŠ€æœ¯æ–‡æ¡£åˆ†å—å™¨...")
        chunker = TechnicalDocChunker()

        print("2ï¸âƒ£  æå–ç»“æ„åŒ–æ–‡æœ¬...")
        structured_text = extractor.extract_structured_text()
        all_text = '\n\n'.join([page['text'] for page in structured_text])

        print(f"   âœ“ æ–‡æœ¬é•¿åº¦: {len(all_text):,} å­—ç¬¦")

        print("3ï¸âƒ£  æ‰§è¡Œç« èŠ‚çº§åˆ†å—ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        chunks = chunker.chunk_by_sections(
            text=all_text,
            structured_text=structured_text,
            metadata={'document_id': document_id}
        )

        print(f"   âœ“ ç”Ÿæˆäº† {len(chunks)} ä¸ªç« èŠ‚/å°èŠ‚å—")

        # ç»Ÿè®¡ä¿¡æ¯
        chapters = [c for c in chunks if c['type'] == 'chapter']
        sections = [c for c in chunks if c['type'] == 'section']
        with_code = [c for c in chunks if c.get('has_code', False)]

        print(f"   âœ“ ç« èŠ‚: {len(chapters)} ä¸ª")
        print(f"   âœ“ å°èŠ‚: {len(sections)} ä¸ª")
        print(f"   âœ“ åŒ…å«ä»£ç : {len(with_code)} ä¸ª")

        # æ˜¾ç¤ºå‰5ä¸ªå—
        print(f"\n   ğŸ“ å‰5ä¸ªç« èŠ‚/å°èŠ‚:")
        for i, chunk in enumerate(chunks[:5], 1):
            title = chunk.get('title', 'Unknown')
            number = chunk.get('number', '')
            level = chunk.get('level', 0)
            char_count = chunk.get('char_count', 0)
            code_count = chunk.get('code_blocks', 0)

            indent = "  " * (level - 1)
            code_info = f", ä»£ç å—:{code_count}" if code_count > 0 else ""
            print(
                f"   {i}. {indent}{number} {title} ({char_count} å­—ç¬¦{code_info})")

        # ========== é˜¶æ®µ 3: é€‰æ‹©éƒ¨åˆ†å—è¿›è¡Œå‘é‡åŒ– ==========
        print("\n" + "="*70)
        print("ğŸ§® é˜¶æ®µ 3: å‘é‡åŒ–ï¼ˆæµ‹è¯•ç”¨ï¼Œåªå¤„ç†å‰20ä¸ªå—ï¼‰")
        print("="*70)

        # åªå–å‰20ä¸ªå—è¿›è¡Œæµ‹è¯•
        test_chunks = chunks[:20]
        print(f"\n1ï¸âƒ£  é€‰æ‹©å‰20ä¸ªå—è¿›è¡Œæµ‹è¯•...")

        print("2ï¸âƒ£  åˆå§‹åŒ– Embeddings æœåŠ¡...")
        embeddings_service = EmbeddingsService(
            model_name="paraphrase-multilingual-MiniLM-L12-v2"
        )

        print("3ï¸âƒ£  ç”Ÿæˆå‘é‡åµŒå…¥...")
        chunks_with_embeddings = embeddings_service.encode_chunks(
            test_chunks,
            batch_size=8
        )
        print(f"   âœ“ å·²ä¸º {len(chunks_with_embeddings)} ä¸ªå—ç”Ÿæˆå‘é‡")
        print(f"   âœ“ å‘é‡ç»´åº¦: {chunks_with_embeddings[0]['embedding_dim']}")

        # ========== é˜¶æ®µ 4: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ ==========
        print("\n" + "="*70)
        print("ğŸ’¾ é˜¶æ®µ 4: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
        print("="*70)

        print("\n1ï¸âƒ£  åˆå§‹åŒ–æ£€ç´¢æœåŠ¡...")
        retrieval_service = RetrievalService(
            collection_name="technical_docs",
            embeddings_service=embeddings_service
        )

        print("2ï¸âƒ£  æ¸…ç©ºæ—§æ•°æ®...")
        retrieval_service.clear_collection()

        print("3ï¸âƒ£  æ·»åŠ ç« èŠ‚åˆ°å‘é‡æ•°æ®åº“...")
        add_result = retrieval_service.add_documents(
            chunks_with_embeddings,
            document_id=document_id,
            batch_size=10
        )
        print(f"   âœ“ æˆåŠŸæ·»åŠ  {add_result['added']} ä¸ªç« èŠ‚")

        # ========== é˜¶æ®µ 5: æŠ€æœ¯æ–‡æ¡£ RAG é—®ç­” ==========
        print("\n" + "="*70)
        print("ğŸ’¬ é˜¶æ®µ 5: æŠ€æœ¯æ–‡æ¡£æ™ºèƒ½é—®ç­”")
        print("="*70)

        print("\n1ï¸âƒ£  åˆå§‹åŒ–æŠ€æœ¯æ–‡æ¡£ RAG æœåŠ¡...")
        rag_service = TechnicalDocRAG(retrieval_service=retrieval_service)

        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "Linuxç³»ç»Ÿä¸­å¦‚ä½•æŸ¥çœ‹æ–‡ä»¶å†…å®¹ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯I/Oé‡å®šå‘ï¼Ÿ",
            "æ–‡ä»¶æƒé™ç®¡ç†çš„åŸºæœ¬å‘½ä»¤æœ‰å“ªäº›ï¼Ÿ",
        ]

        for i, question in enumerate(test_questions[:2], 1):  # åªæµ‹è¯•å‰2ä¸ªé—®é¢˜
            print(f"\n{i}ï¸âƒ£  é—®é¢˜: {question}")
            print("   ğŸ¤– Gemini æ­£åœ¨åˆ†æç›¸å…³ç« èŠ‚...")

            result = await rag_service.answer_knowledge_point(
                question=question,
                document_id=document_id,
                n_contexts=2,
                language="zh",
                include_code=True
            )

            print(f"\n   ğŸ“ å›ç­”:")
            answer_preview = result['answer'][:500] + \
                "..." if len(result['answer']) > 500 else result['answer']
            print(f"   {answer_preview}")

            print(f"\n   ğŸ“š å‚è€ƒç« èŠ‚: {result['source_count']} ä¸ª")
            for j, chapter in enumerate(result['chapters'], 1):
                number = chapter.get('number', '')
                title = chapter.get('title', '')
                has_code = "ğŸ”§" if chapter.get('has_code') else ""
                print(f"      {j}. {number} {title} {has_code}")

        # ========== é˜¶æ®µ 6: ä»£ç è§£é‡Š ==========
        print("\n" + "="*70)
        print("ğŸ”§ é˜¶æ®µ 6: ä»£ç ç‰‡æ®µè§£é‡Š")
        print("="*70)

        # ä»æ–‡æ¡£ä¸­æ‰¾ä¸€æ®µä»£ç 
        code_chunks = [c for c in test_chunks if c.get('has_code', False)]
        if code_chunks:
            print("\n1ï¸âƒ£  æ‰¾åˆ°åŒ…å«ä»£ç çš„ç« èŠ‚...")
            code_chunk = code_chunks[0]

            # æå–ä»£ç ç‰‡æ®µï¼ˆç®€å•ç¤ºä¾‹ï¼‰
            code_sample = code_chunk['text'][:300]
            print(
                f"   ç« èŠ‚: {code_chunk.get('number', '')} {code_chunk.get('title', '')}")
            print(f"   ä»£ç é¢„è§ˆ:\n{code_sample}\n")

            print("2ï¸âƒ£  è¯·æ±‚ä»£ç è§£é‡Š...")
            code_result = await rag_service.explain_code_snippet(
                code=code_sample,
                document_id=document_id,
                language="zh"
            )

            print(f"\n   ğŸ“– ä»£ç è§£é‡Š:")
            explanation_preview = code_result['explanation'][:400] + "..." if len(
                code_result['explanation']) > 400 else code_result['explanation']
            print(f"   {explanation_preview}")
        else:
            print("\n   â„¹ï¸  æµ‹è¯•çš„å‰20ä¸ªå—ä¸­æœªæ‰¾åˆ°ä»£ç å—")

        # ========== æµ‹è¯•å®Œæˆ ==========
        print("\n" + "="*70)
        print("âœ… æŠ€æœ¯æ–‡æ¡£å¤„ç†æµ‹è¯•å®Œæˆï¼")
        print("="*70)

        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print(f"   â€¢ æ–‡æ¡£é¡µæ•°: {metadata['page_count']}")
        print(f"   â€¢ ç« èŠ‚æ€»æ•°: {len(chunks)}")
        print(f"   â€¢ æµ‹è¯•å—æ•°: {len(test_chunks)}")
        print(f"   â€¢ å‘é‡åŒ–: âœ… {len(chunks_with_embeddings)} ä¸ª")
        print(f"   â€¢ æ•°æ®åº“: âœ… {add_result['added']} æ¡")
        print(f"   â€¢ RAG é—®ç­”: âœ… æ­£å¸¸")
        print(f"   â€¢ ä»£ç è§£é‡Š: âœ… æ­£å¸¸")

        print("\nğŸ‰ æŠ€æœ¯æ–‡æ¡£ RAG ç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼")

        print("\nğŸ’¡ æç¤º:")
        print("   - å®Œæ•´æ–‡æ¡£æœ‰ 1063 é¡µï¼Œæœ¬æ¬¡åªæµ‹è¯•äº†å‰20ä¸ªç« èŠ‚")
        print("   - è¦å¤„ç†å®Œæ•´æ–‡æ¡£ï¼Œéœ€è¦æ›´é•¿æ—¶é—´å’Œæ›´å¤šèµ„æº")
        print("   - å»ºè®®æ ¹æ®éœ€è¦é€‰æ‹©ç‰¹å®šç« èŠ‚è¿›è¡Œå¤„ç†")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_technical_doc_pipeline())
