"""
æµ‹è¯• PDF è§£æåŠŸèƒ½
ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ è®ºæ–‡.pdf è¿›è¡Œæµ‹è¯•
"""
from loguru import logger
from app.services.pdf import PDFParser, PDFExtractor, PDFChunker
import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "backend"))


def test_pdf_parser():
    """æµ‹è¯• PDF è§£æå™¨"""
    print("\n" + "="*60)
    print("ğŸ“„ æµ‹è¯• PDF è§£æå™¨")
    print("="*60)

    pdf_path = Path(__file__).parent / "è®ºæ–‡.pdf"

    if not pdf_path.exists():
        print(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    try:
        # åˆ›å»ºè§£æå™¨
        parser = PDFParser(pdf_path)

        # 1. æå–å…ƒæ•°æ®
        print("\n1ï¸âƒ£  æå–å…ƒæ•°æ®...")
        metadata = parser.get_metadata()
        print(f"   é¡µæ•°: {metadata['page_count']}")
        print(f"   æ ‡é¢˜: {metadata.get('title', 'N/A')}")
        print(f"   ä½œè€…: {metadata.get('author', 'N/A')}")

        # 2. æå–æ–‡æœ¬ï¼ˆå‰3é¡µï¼‰
        print("\n2ï¸âƒ£  æå–æ–‡æœ¬ï¼ˆå‰3é¡µï¼‰...")
        text_by_page = parser.extract_text(
            engine="pymupdf", page_numbers=[0, 1, 2])
        for page_num, text in text_by_page.items():
            print(f"   ç¬¬ {page_num + 1} é¡µ: {len(text)} å­—ç¬¦")
            print(f"   é¢„è§ˆ: {text[:100]}...")

        # 3. æå–è¡¨æ ¼
        print("\n3ï¸âƒ£  æå–è¡¨æ ¼...")
        tables = parser.extract_tables()
        print(f"   æ‰¾åˆ° {len(tables)} ä¸ªé¡µé¢åŒ…å«è¡¨æ ¼")
        for page_num, page_tables in tables.items():
            print(f"   ç¬¬ {page_num + 1} é¡µ: {len(page_tables)} ä¸ªè¡¨æ ¼")

        # 4. æå–å›¾ç‰‡ä¿¡æ¯
        print("\n4ï¸âƒ£  æå–å›¾ç‰‡...")
        images = parser.extract_images()
        total_images = sum(len(imgs) for imgs in images.values())
        print(f"   æ‰¾åˆ° {total_images} å¼ å›¾ç‰‡")

        print("\nâœ… PDF è§£æå™¨æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def test_pdf_extractor():
    """æµ‹è¯• PDF æå–å™¨"""
    print("\n" + "="*60)
    print("ğŸ“ æµ‹è¯• PDF æå–å™¨")
    print("="*60)

    pdf_path = Path(__file__).parent / "è®ºæ–‡.pdf"

    if not pdf_path.exists():
        print(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    try:
        # åˆ›å»ºæå–å™¨
        extractor = PDFExtractor(pdf_path)

        # 1. æå–ç»“æ„åŒ–æ–‡æœ¬
        print("\n1ï¸âƒ£  æå–ç»“æ„åŒ–æ–‡æœ¬...")
        structured_text = extractor.extract_structured_text()
        print(f"   å…± {len(structured_text)} ä¸ªé¡µé¢æœ‰å†…å®¹")
        if structured_text:
            first_page = structured_text[0]
            print(f"   ç¬¬ä¸€é¡µå­—ç¬¦æ•°: {first_page['char_count']}")
            print(f"   ç¬¬ä¸€é¡µå•è¯æ•°: {first_page['word_count']}")
            print(f"   é¦–è¡Œ: {first_page.get('first_line', 'N/A')[:50]}...")

        # 2. æå–ç« èŠ‚
        print("\n2ï¸âƒ£  æå–æ–‡æ¡£ç« èŠ‚...")
        sections = extractor.extract_sections()
        print(f"   æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚")
        for i, section in enumerate(sections[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(
                f"   {i+1}. {section['title'][:40]}... (ç¬¬ {section['start_page']} é¡µ)")

        # 3. æå–å¢å¼ºå…ƒæ•°æ®
        print("\n3ï¸âƒ£  æå–å¢å¼ºå…ƒæ•°æ®...")
        enhanced_meta = extractor.extract_metadata_enhanced()
        content_analysis = enhanced_meta['content_analysis']
        print(f"   æ€»å­—ç¬¦æ•°: {content_analysis['total_characters']}")
        print(f"   ä¸­æ–‡å­—ç¬¦: {content_analysis['chinese_characters']}")
        print(f"   è‹±æ–‡å­—ç¬¦: {content_analysis['english_characters']}")
        print(f"   æ£€æµ‹è¯­è¨€: {content_analysis['detected_language']}")
        print(f"   è¯­è¨€ç½®ä¿¡åº¦: {content_analysis['language_confidence']:.2%}")

        print("\nâœ… PDF æå–å™¨æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def test_pdf_chunker():
    """æµ‹è¯• PDF åˆ†å—å™¨"""
    print("\n" + "="*60)
    print("âœ‚ï¸  æµ‹è¯• PDF åˆ†å—å™¨")
    print("="*60)

    pdf_path = Path(__file__).parent / "è®ºæ–‡.pdf"

    if not pdf_path.exists():
        print(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    try:
        # åˆ›å»ºæå–å™¨è·å–æ–‡æœ¬
        extractor = PDFExtractor(pdf_path)
        structured_text = extractor.extract_structured_text()

        if not structured_text:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å†…å®¹")
            return

        # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬
        all_text = '\n\n'.join([page['text'] for page in structured_text])

        # åˆ›å»ºåˆ†å—å™¨
        chunker = PDFChunker(chunk_size=1000, chunk_overlap=200)

        # 1. å›ºå®šå¤§å°åˆ†å—
        print("\n1ï¸âƒ£  å›ºå®šå¤§å°åˆ†å—...")
        fixed_chunks = chunker.chunk_by_fixed_size(all_text)
        print(f"   ç”Ÿæˆ {len(fixed_chunks)} ä¸ªå—")
        if fixed_chunks:
            print(f"   ç¬¬ä¸€å—: {fixed_chunks[0]['char_count']} å­—ç¬¦")
            print(f"   é¢„è§ˆ: {fixed_chunks[0]['text'][:100]}...")

        # 2. æ®µè½åˆ†å—
        print("\n2ï¸âƒ£  æ®µè½åˆ†å—...")
        para_chunks = chunker.chunk_by_paragraph(all_text)
        print(f"   ç”Ÿæˆ {len(para_chunks)} ä¸ªå—")
        if para_chunks:
            print(
                f"   å¹³å‡å—å¤§å°: {sum(c['char_count'] for c in para_chunks) / len(para_chunks):.0f} å­—ç¬¦")

        # 3. é¡µé¢åˆ†å—
        print("\n3ï¸âƒ£  é¡µé¢åˆ†å—...")
        page_chunks = chunker.chunk_by_page(structured_text)
        print(f"   ç”Ÿæˆ {len(page_chunks)} ä¸ªå—")
        if page_chunks:
            print(f"   ç¬¬ä¸€å—é¡µç : {page_chunks[0]['page_numbers']}")

        # 4. æ™ºèƒ½æ··åˆåˆ†å—
        print("\n4ï¸âƒ£  æ™ºèƒ½æ··åˆåˆ†å—...")
        smart_chunks = chunker.chunk_smart(all_text, strategy="hybrid")
        print(f"   ç”Ÿæˆ {len(smart_chunks)} ä¸ªå—")

        print("\nâœ… PDF åˆ†å—å™¨æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def test_comprehensive():
    """ç»¼åˆæµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸš€ ç»¼åˆæµ‹è¯•ï¼šå®Œæ•´æå–æµç¨‹")
    print("="*60)

    pdf_path = Path(__file__).parent / "è®ºæ–‡.pdf"

    if not pdf_path.exists():
        print(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    try:
        # åˆ›å»ºæå–å™¨
        extractor = PDFExtractor(pdf_path)

        # æ‰§è¡Œå®Œæ•´æå–
        print("\nğŸ“Š æ‰§è¡Œå®Œæ•´æå–...")
        result = extractor.extract_all()

        # æ˜¾ç¤ºæ‘˜è¦
        summary = result['summary']
        print(f"\nğŸ“ˆ æå–æ‘˜è¦:")
        print(f"   æ€»é¡µæ•°: {summary['total_pages']}")
        print(f"   æœ‰æ–‡æœ¬çš„é¡µé¢: {summary['pages_with_text']}")
        print(f"   ç« èŠ‚æ•°: {summary['total_sections']}")
        print(f"   è¡¨æ ¼æ•°: {summary['total_tables']}")
        print(f"   æ€»å­—ç¬¦æ•°: {summary['total_characters']}")
        print(f"   æ£€æµ‹è¯­è¨€: {summary['detected_language']}")

        # æ˜¾ç¤ºéƒ¨åˆ†ç« èŠ‚
        if result['sections']:
            print(f"\nğŸ“‘ æ–‡æ¡£ç« èŠ‚ï¼ˆå‰5ä¸ªï¼‰:")
            for i, section in enumerate(result['sections'][:5]):
                print(f"   {i+1}. {section['title'][:50]}...")

        # æ‰§è¡Œåˆ†å—
        print(f"\nâœ‚ï¸  å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—...")
        chunker = PDFChunker(chunk_size=1000, chunk_overlap=200)
        all_text = '\n\n'.join([page['text']
                               for page in result['structured_text']])
        chunks = chunker.chunk_smart(all_text, strategy="hybrid")
        print(f"   ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")

        print("\nâœ… ç»¼åˆæµ‹è¯•å®Œæˆï¼PDF è§£ææµç¨‹å·¥ä½œæ­£å¸¸ã€‚")

    except Exception as e:
        print(f"\nâŒ ç»¼åˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ§ª IntelliPDF - PDF å¤„ç†æ¨¡å—æµ‹è¯•")
    print("="*60)

    test_pdf_parser()
    test_pdf_extractor()
    test_pdf_chunker()
    test_comprehensive()

    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
