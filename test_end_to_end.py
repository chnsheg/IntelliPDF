"""
ç«¯åˆ°ç«¯æµ‹è¯•ï¼šPDF ä¸Šä¼  -> è§£æ -> å‘é‡åŒ– -> RAG é—®ç­”
ä½¿ç”¨ è®ºæ–‡.pdf è¿›è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
"""
from loguru import logger
from app.services.ai import EmbeddingsService, RetrievalService, LLMService
from app.services.pdf import PDFExtractor, PDFChunker
import sys
import os
import asyncio
from pathlib import Path

# ä¿å­˜åŸå§‹ç›®å½•
original_dir = os.getcwd()

# åˆ‡æ¢åˆ° backend ç›®å½•ä»¥åŠ è½½ .env
backend_dir = Path(__file__).parent / "backend"
os.chdir(backend_dir)
sys.path.insert(0, str(backend_dir))


async def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„ PDF å¤„ç†å’Œé—®ç­”æµç¨‹"""
    print("\n" + "="*70)
    print("ğŸš€ IntelliPDF ç«¯åˆ°ç«¯æµ‹è¯•")
    print("="*70)

    # PDF åœ¨åŸå§‹ç›®å½•
    pdf_path = Path(original_dir) / "è®ºæ–‡.pdf"

    if not pdf_path.exists():
        print(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return

    document_id = "test_paper_001"

    try:
        # ========== é˜¶æ®µ 1: PDF è§£æ ==========
        print("\n" + "="*70)
        print("ğŸ“„ é˜¶æ®µ 1: PDF è§£æä¸æå–")
        print("="*70)

        print("\n1ï¸âƒ£  åˆ›å»º PDF æå–å™¨...")
        extractor = PDFExtractor(pdf_path)

        print("2ï¸âƒ£  æå–æ–‡æ¡£å…ƒæ•°æ®...")
        metadata = extractor.extract_metadata_enhanced()
        print(f"   âœ“ æ€»é¡µæ•°: {metadata['page_count']}")
        print(f"   âœ“ æ€»å­—ç¬¦æ•°: {metadata['content_analysis']['total_characters']}")
        print(
            f"   âœ“ æ£€æµ‹è¯­è¨€: {metadata['content_analysis']['detected_language']}")

        print("3ï¸âƒ£  æå–ç»“æ„åŒ–æ–‡æœ¬...")
        structured_text = extractor.extract_structured_text()
        print(f"   âœ“ æå–äº† {len(structured_text)} ä¸ªé¡µé¢çš„æ–‡æœ¬")

        # ========== é˜¶æ®µ 2: æ–‡æ¡£åˆ†å— ==========
        print("\n" + "="*70)
        print("âœ‚ï¸  é˜¶æ®µ 2: æ™ºèƒ½æ–‡æ¡£åˆ†å—")
        print("="*70)

        print("\n1ï¸âƒ£  åˆ›å»ºåˆ†å—å™¨...")
        chunker = PDFChunker(chunk_size=1000, chunk_overlap=200)

        print("2ï¸âƒ£  æ‰§è¡Œæ™ºèƒ½åˆ†å—...")
        all_text = '\n\n'.join([page['text'] for page in structured_text])
        chunks = chunker.chunk_smart(all_text, strategy="hybrid")
        print(f"   âœ“ ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æ¡£å—")
        print(
            f"   âœ“ å¹³å‡å—å¤§å°: {sum(c['char_count'] for c in chunks) / len(chunks):.0f} å­—ç¬¦")

        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå—çš„é¢„è§ˆ
        if chunks:
            first_chunk = chunks[0]
            print(f"\n   ğŸ“ ç¬¬ä¸€ä¸ªå—é¢„è§ˆ:")
            print(f"   {first_chunk['text'][:150]}...")

        # ========== é˜¶æ®µ 3: å‘é‡åŒ– ==========
        print("\n" + "="*70)
        print("ğŸ§® é˜¶æ®µ 3: ç”Ÿæˆå‘é‡åµŒå…¥")
        print("="*70)

        print("\n1ï¸âƒ£  åˆå§‹åŒ– Embeddings æœåŠ¡...")
        embeddings_service = EmbeddingsService(
            model_name="paraphrase-multilingual-MiniLM-L12-v2"
        )

        print("2ï¸âƒ£  ä¸ºæ–‡æ¡£å—ç”Ÿæˆå‘é‡...")
        # åªå¤„ç†å‰20ä¸ªå—ä»¥èŠ‚çœæ—¶é—´
        chunks_to_process = chunks[:20]
        chunks_with_embeddings = embeddings_service.encode_chunks(
            chunks_to_process,
            batch_size=8
        )
        print(f"   âœ“ å·²ä¸º {len(chunks_with_embeddings)} ä¸ªå—ç”Ÿæˆå‘é‡")
        print(f"   âœ“ å‘é‡ç»´åº¦: {chunks_with_embeddings[0]['embedding_dim']}")

        # ========== é˜¶æ®µ 4: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ ==========
        print("\n" + "="*70)
        print("ğŸ’¾ é˜¶æ®µ 4: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
        print("="*70)

        print("\n1ï¸âƒ£  åˆå§‹åŒ–æ£€ç´¢æœåŠ¡...")
        retrieval_service = RetrievalService(
            collection_name="test_collection",
            embeddings_service=embeddings_service
        )

        print("2ï¸âƒ£  æ¸…ç©ºæ—§æ•°æ®...")
        retrieval_service.clear_collection()

        print("3ï¸âƒ£  æ·»åŠ æ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“...")
        add_result = retrieval_service.add_documents(
            chunks_with_embeddings,
            document_id=document_id,
            batch_size=10
        )
        print(f"   âœ“ æˆåŠŸæ·»åŠ  {add_result['added']} ä¸ªå—")

        print("4ï¸âƒ£  è·å–é›†åˆç»Ÿè®¡...")
        stats = retrieval_service.get_collection_stats()
        print(f"   âœ“ é›†åˆåç§°: {stats['collection_name']}")
        print(f"   âœ“ æ€»å—æ•°: {stats['total_chunks']}")
        print(f"   âœ“ å‘é‡ç»´åº¦: {stats['embedding_dimension']}")

        # ========== é˜¶æ®µ 5: å‘é‡æ£€ç´¢æµ‹è¯• ==========
        print("\n" + "="*70)
        print("ğŸ” é˜¶æ®µ 5: å‘é‡æ£€ç´¢æµ‹è¯•")
        print("="*70)

        test_queries = [
            "è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦ç ”ç©¶å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è®ºæ–‡ä¸­ä½¿ç”¨äº†ä»€ä¹ˆæŠ€æœ¯æ–¹æ³•ï¼Ÿ",
            "ç ”ç©¶çš„ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ"
        ]

        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}ï¸âƒ£  æŸ¥è¯¢: {query}")
            search_results = retrieval_service.search(query, n_results=3)
            print(f"   âœ“ æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ç»“æœ")

            if search_results:
                top_result = search_results[0]
                print(f"   ğŸ“„ æœ€ç›¸å…³ç»“æœé¢„è§ˆ:")
                print(f"   {top_result['text'][:150]}...")
                if top_result.get('distance') is not None:
                    relevance = 1 - top_result['distance']
                    print(f"   ç›¸å…³åº¦: {relevance:.2%}")

        # ========== é˜¶æ®µ 6: RAG é—®ç­” ==========
        print("\n" + "="*70)
        print("ğŸ’¬ é˜¶æ®µ 6: RAG æ™ºèƒ½é—®ç­”")
        print("="*70)

        print("\n1ï¸âƒ£  åˆå§‹åŒ– LLM æœåŠ¡...")
        llm_service = LLMService(retrieval_service=retrieval_service)

        qa_questions = [
            "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹è¿™ç¯‡è®ºæ–‡çš„ç ”ç©¶å†…å®¹ã€‚",
            "è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]

        for i, question in enumerate(qa_questions, 1):
            print(f"\n{i}ï¸âƒ£  é—®é¢˜: {question}")
            print("   ğŸ¤– Gemini æ­£åœ¨æ€è€ƒ...")

            result = await llm_service.answer_question(
                question=question,
                document_id=document_id,
                n_contexts=3,
                language="zh"
            )

            print(f"\n   ğŸ“ å›ç­”:")
            print(f"   {result['answer']}")
            print(f"\n   ğŸ“š ä½¿ç”¨äº† {result['source_count']} ä¸ªæ–‡æ¡£ç‰‡æ®µä½œä¸ºå‚è€ƒ")

        # ========== é˜¶æ®µ 7: æ–‡æ¡£æ€»ç»“ ==========
        print("\n" + "="*70)
        print("ğŸ“Š é˜¶æ®µ 7: æ–‡æ¡£æ™ºèƒ½æ€»ç»“")
        print("="*70)

        print("\n1ï¸âƒ£  ç”Ÿæˆæ–‡æ¡£æ€»ç»“...")
        summary_result = await llm_service.summarize_document(
            document_id=document_id,
            max_chunks=5,
            language="zh"
        )

        print(f"\n   ğŸ“„ æ–‡æ¡£æ€»ç»“:")
        print(f"   {summary_result['summary']}")
        print(f"\n   â„¹ï¸  åŸºäº {summary_result['chunk_count']} ä¸ªæ–‡æ¡£å—ç”Ÿæˆ")

        # ========== é˜¶æ®µ 8: å…³é”®è¯æå– ==========
        print("\n" + "="*70)
        print("ğŸ·ï¸  é˜¶æ®µ 8: å…³é”®è¯æå–")
        print("="*70)

        print("\n1ï¸âƒ£  æå–æ–‡æ¡£å…³é”®è¯...")
        keywords_result = await llm_service.extract_keywords(
            document_id=document_id,
            max_keywords=10,
            language="zh"
        )

        print(f"\n   ğŸ”‘ æå–åˆ° {keywords_result['count']} ä¸ªå…³é”®è¯:")
        for i, keyword in enumerate(keywords_result['keywords'], 1):
            print(f"   {i}. {keyword}")

        # ========== æµ‹è¯•å®Œæˆ ==========
        print("\n" + "="*70)
        print("âœ… ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("="*70)

        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print(f"   â€¢ PDF è§£æ: âœ… {metadata['page_count']} é¡µ")
        print(f"   â€¢ æ–‡æ¡£åˆ†å—: âœ… {len(chunks)} ä¸ªå—")
        print(f"   â€¢ å‘é‡ç”Ÿæˆ: âœ… {len(chunks_with_embeddings)} ä¸ªå‘é‡")
        print(f"   â€¢ æ•°æ®åº“å­˜å‚¨: âœ… {add_result['added']} æ¡è®°å½•")
        print(f"   â€¢ å‘é‡æ£€ç´¢: âœ… æ­£å¸¸")
        print(f"   â€¢ RAG é—®ç­”: âœ… æ­£å¸¸")
        print(f"   â€¢ æ–‡æ¡£æ€»ç»“: âœ… æ­£å¸¸")
        print(f"   â€¢ å…³é”®è¯æå–: âœ… æ­£å¸¸")

        print("\nğŸ‰ IntelliPDF æ ¸å¿ƒåŠŸèƒ½å…¨éƒ¨æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_full_pipeline())
